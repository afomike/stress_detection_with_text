# -*- coding: utf-8 -*-
"""stress.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/168J99DFUcxacBtxzNHsLa5f7-J-yKP2f
"""

import pandas as pd
import numpy as np
import seaborn as sns
from wordcloud import STOPWORDS
from PIL import Image
import matplotlib.pyplot as plt
from wordcloud import WordCloud

from textblob import TextBlob
import nltk
import re
stemmer = nltk.SnowballStemmer("english")
from nltk.corpus import stopwords
import string

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings("ignore")

"""Read the Dataset"""

dataset = pd.read_csv("Stress.csv")

"""Data Exploration"""

print("Rows :", dataset.shape[0])
print("Columns :", dataset.shape[1])
print()
print("Categorical columns :")
print(dataset.select_dtypes(include=['object']).apply(pd.Series.nunique, axis=0))
print()
print("Description :")
print(dataset.info())
print()
print("missing values:")
print(dataset.isnull().sum())

""" Data Sample"""

dataset.sample()

#CHECKING FOR CORRELATION

corrmat =dataset.corr()
top_corr_features=corrmat.index
plt.figure(figsize=(15,15))
g=sns.heatmap(dataset[top_corr_features].corr(),annot=True, cmap="RdYlGn")

#Positive Sentiment
print("Positive Sentiment :")
print("Polarity : ",TextBlob("greatest").polarity)
print("Sentiment : ",TextBlob("greatest").sentiment)
print()
#Negative Sentiment
print("Negative Sentiment :")
print("Polarity : ",TextBlob("worst").polarity)
print("Sentiment : ",TextBlob("worst").sentiment)

#Sentiment Detection
def mood(txt):
    return TextBlob(txt).sentiment.polarity

#Get text
texts = dataset[["text"]]

#Adding Mood Detection as column
texts["sentiment"] = texts["text"].apply(mood)
texts.head()

print("Value Exploration :")
print("Minimum Sentiment :", texts["sentiment"].min())
print("Maximum Sentiment :", texts["sentiment"].max())
print()
print("Counts :")
print(texts.sentiment.value_counts())

nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords = set(stopwords.words("english"))
def removal(text):
    text = str(text).lower()
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = [w for w in text.split(' ') if w not in stopwords]
    text=" ".join(text)
    text = [stemmer.stem(word) for word in text.split(' ')]
    text=" ".join(text)
    return text
texts["text"] = texts["text"].apply(removal)
texts["text"]

#Wordcloud
def wc(data,bgcolor):
    plt.figure(figsize=(30,30))
    wc=WordCloud(background_color=bgcolor,stopwords=STOPWORDS)
    wc.generate(' '.join(data))
    plt.imshow(wc)
    plt.axis("off")
wc(texts.text,'grey')

#Adding meaning to text from dataset
texts["meaning"] = dataset["label"].map({0:"Unstressed", 1:"Stressed"})
#posts = posts[["text", "meaning"]]
texts.head()

#Applying sentiment
texts["sentiment"] = texts["text"].apply(mood)
texts.head()

#sns.displot(x=posts.meaning, kind='kde')
sns.countplot(x='meaning',data = texts, palette= ["#e1c0b6", "#a3b8c8"])

"""Modelling """

#Assign variables; x = features & y = target
x = texts.text
y = texts.meaning

vect=CountVectorizer(stop_words="english")
x=vect.fit_transform(x)

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2, random_state=0 )

from sklearn.naive_bayes import MultinomialNB
mb=MultinomialNB()
mb1 = mb.fit(x_train,y_train)
mb2=mb.predict(x_test)
print("Model Accuracy is {p}%".format(p =round (accuracy_score(mb2,y_test)*100, 2)))

from sklearn.linear_model import LogisticRegression
log=LogisticRegression()
log1 = log.fit(x_train, y_train)
log2=log.predict(x_test)
# score = m2.score(x_test,y_test)
#accuracy_score(m2,y_test)
# print("Model Accuracy is {p}%".format(p =round (score*100, 2)))
print("Model Accuracy is {p}%".format(p =round (accuracy_score(y_test,log2)*100, 2)))

from sklearn.tree import DecisionTreeClassifier
d=DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
d.fit(x_train,y_train)
m3=d.predict(x_test)
print("Model Accuracy is {p}%".format(p =round (accuracy_score(y_test,m3)*100, 2)))

from sklearn.svm import SVC
svc = SVC(kernel = 'linear', random_state = 43)
svc.fit(x_train,y_train)
m5 = svc.predict(x_test)
print("Model Accuracy is {p}%".format(p =round (accuracy_score(y_test,m5)*100, 2)))

#Using Naive Bayes1
prompt = "Its a beautiful day & i can't stop myself from smiling"
p = vect.transform([prompt]).toarray()
output = mb.predict(p)
print(output)

#Using Naive Bayes2
prompt = "He felt underappreciated and resented his job"
p = vect.transform([prompt]).toarray()
output = mb.predict(p)
print(output)

#support vector machine 1
prompt = "He felt underappreciated and resented his job"
p = vect.transform([prompt]).toarray()
output = svc.predict(p)
print(output)

#support vector machine 2
prompt = "Its a beautiful day & i can't stop myself from smiling"
p = vect.transform([prompt]).toarray()
output = svc.predict(p)
print(output)

import pickle

Pkl_Filename = "Stress.pkl"  

with open(Pkl_Filename, 'wb') as file:  
    pickle.dump(svc, file)
    
with open('vectorizer.pkl', 'wb') as file:
    pickle.dump(vect, file)

# load the model back from file

with open(Pkl_Filename, 'rb') as file:  
    Pickled_LR_Model = pickle.load(file)

Pickled_LR_Model